{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1907aefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries for overall analysis\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_rows', None)\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "sns.set_style(\"whitegrid\");\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import pickle as pkl\n",
    "import tqdm as tqdm\n",
    "from random import choices\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import roc_auc_score,log_loss\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV,GridSearchCV\n",
    "from tqdm import tqdm\n",
    "from lightgbm import LGBMClassifier,LGBMRegressor\n",
    "from sklearn.svm import SVC\n",
    "import joblib\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e4e2ad08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SK_ID_CURR</th>\n",
       "      <th>NAME_CONTRACT_TYPE</th>\n",
       "      <th>CODE_GENDER</th>\n",
       "      <th>FLAG_OWN_CAR</th>\n",
       "      <th>FLAG_OWN_REALTY</th>\n",
       "      <th>CNT_CHILDREN</th>\n",
       "      <th>AMT_INCOME_TOTAL</th>\n",
       "      <th>AMT_CREDIT</th>\n",
       "      <th>AMT_ANNUITY</th>\n",
       "      <th>AMT_GOODS_PRICE</th>\n",
       "      <th>...</th>\n",
       "      <th>FLAG_DOCUMENT_18</th>\n",
       "      <th>FLAG_DOCUMENT_19</th>\n",
       "      <th>FLAG_DOCUMENT_20</th>\n",
       "      <th>FLAG_DOCUMENT_21</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_HOUR</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_DAY</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_WEEK</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_MON</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_QRT</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_YEAR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100001</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>568800.0</td>\n",
       "      <td>20560.5</td>\n",
       "      <td>450000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100005</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>99000.0</td>\n",
       "      <td>222768.0</td>\n",
       "      <td>17370.0</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100013</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>M</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>202500.0</td>\n",
       "      <td>663264.0</td>\n",
       "      <td>69777.0</td>\n",
       "      <td>630000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100028</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>2</td>\n",
       "      <td>315000.0</td>\n",
       "      <td>1575000.0</td>\n",
       "      <td>49018.5</td>\n",
       "      <td>1575000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100038</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>M</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>1</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>625500.0</td>\n",
       "      <td>32067.0</td>\n",
       "      <td>625500.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>100042</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>F</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>270000.0</td>\n",
       "      <td>959688.0</td>\n",
       "      <td>34600.5</td>\n",
       "      <td>810000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>100057</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>M</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>2</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>499221.0</td>\n",
       "      <td>22117.5</td>\n",
       "      <td>373500.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>100065</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>166500.0</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>14220.0</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>100066</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>315000.0</td>\n",
       "      <td>364896.0</td>\n",
       "      <td>28957.5</td>\n",
       "      <td>315000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>100067</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>F</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>1</td>\n",
       "      <td>162000.0</td>\n",
       "      <td>45000.0</td>\n",
       "      <td>5337.0</td>\n",
       "      <td>45000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 121 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   SK_ID_CURR NAME_CONTRACT_TYPE CODE_GENDER FLAG_OWN_CAR FLAG_OWN_REALTY  \\\n",
       "0      100001         Cash loans           F            N               Y   \n",
       "1      100005         Cash loans           M            N               Y   \n",
       "2      100013         Cash loans           M            Y               Y   \n",
       "3      100028         Cash loans           F            N               Y   \n",
       "4      100038         Cash loans           M            Y               N   \n",
       "5      100042         Cash loans           F            Y               Y   \n",
       "6      100057         Cash loans           M            Y               Y   \n",
       "7      100065         Cash loans           M            N               Y   \n",
       "8      100066         Cash loans           F            N               Y   \n",
       "9      100067         Cash loans           F            Y               Y   \n",
       "\n",
       "   CNT_CHILDREN  AMT_INCOME_TOTAL  AMT_CREDIT  AMT_ANNUITY  AMT_GOODS_PRICE  \\\n",
       "0             0          135000.0    568800.0      20560.5         450000.0   \n",
       "1             0           99000.0    222768.0      17370.0         180000.0   \n",
       "2             0          202500.0    663264.0      69777.0         630000.0   \n",
       "3             2          315000.0   1575000.0      49018.5        1575000.0   \n",
       "4             1          180000.0    625500.0      32067.0         625500.0   \n",
       "5             0          270000.0    959688.0      34600.5         810000.0   \n",
       "6             2          180000.0    499221.0      22117.5         373500.0   \n",
       "7             0          166500.0    180000.0      14220.0         180000.0   \n",
       "8             0          315000.0    364896.0      28957.5         315000.0   \n",
       "9             1          162000.0     45000.0       5337.0          45000.0   \n",
       "\n",
       "   ... FLAG_DOCUMENT_18 FLAG_DOCUMENT_19 FLAG_DOCUMENT_20 FLAG_DOCUMENT_21  \\\n",
       "0  ...                0                0                0                0   \n",
       "1  ...                0                0                0                0   \n",
       "2  ...                0                0                0                0   \n",
       "3  ...                0                0                0                0   \n",
       "4  ...                0                0                0                0   \n",
       "5  ...                0                0                0                0   \n",
       "6  ...                0                0                0                0   \n",
       "7  ...                0                0                0                0   \n",
       "8  ...                0                0                0                0   \n",
       "9  ...                0                0                0                0   \n",
       "\n",
       "  AMT_REQ_CREDIT_BUREAU_HOUR  AMT_REQ_CREDIT_BUREAU_DAY  \\\n",
       "0                        0.0                        0.0   \n",
       "1                        0.0                        0.0   \n",
       "2                        0.0                        0.0   \n",
       "3                        0.0                        0.0   \n",
       "4                        NaN                        NaN   \n",
       "5                        0.0                        0.0   \n",
       "6                        0.0                        0.0   \n",
       "7                        0.0                        0.0   \n",
       "8                        0.0                        0.0   \n",
       "9                        0.0                        0.0   \n",
       "\n",
       "   AMT_REQ_CREDIT_BUREAU_WEEK  AMT_REQ_CREDIT_BUREAU_MON  \\\n",
       "0                         0.0                        0.0   \n",
       "1                         0.0                        0.0   \n",
       "2                         0.0                        0.0   \n",
       "3                         0.0                        0.0   \n",
       "4                         NaN                        NaN   \n",
       "5                         0.0                        0.0   \n",
       "6                         0.0                        0.0   \n",
       "7                         0.0                        0.0   \n",
       "8                         0.0                        0.0   \n",
       "9                         0.0                        0.0   \n",
       "\n",
       "   AMT_REQ_CREDIT_BUREAU_QRT  AMT_REQ_CREDIT_BUREAU_YEAR  \n",
       "0                        0.0                         0.0  \n",
       "1                        0.0                         3.0  \n",
       "2                        1.0                         4.0  \n",
       "3                        0.0                         3.0  \n",
       "4                        NaN                         NaN  \n",
       "5                        1.0                         2.0  \n",
       "6                        0.0                         1.0  \n",
       "7                        0.0                         2.0  \n",
       "8                        0.0                         5.0  \n",
       "9                        0.0                         2.0  \n",
       "\n",
       "[10 rows x 121 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test=pd.read_csv(r'home-credit-default-risk/application_test.csv')\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c703299",
   "metadata": {},
   "outputs": [],
   "source": [
    "## preprocessing the train and test file\n",
    "### ** There is a catch actually we need to preprocess the train and test data together as we are filling NA and using label \n",
    "## encoding ,Once the train data is transfomed ,The label ENcoder wont work well,SO we will pre process one by one\n",
    "#we will create function for this\n",
    "\n",
    "\n",
    "\n",
    "def preporcess_application_data(test):\n",
    "    \"\"\"\n",
    "    This function take application_train|test .csv and just do some normal preprocessing\n",
    "    These are just primary preprocessing\n",
    "    \"\"\"\n",
    "    #Step 1. drop the non important columns\n",
    "    \n",
    "    #In EDA we have seen some column which were no use for predicting Target Varibale,due to low variance,and multicollineraity\n",
    "    #So we will drop it\n",
    "    \n",
    "    col_to_drop=['FLAG_MOBIL','FLAG_DOCUMENT_2','FLAG_DOCUMENT_4','FLAG_DOCUMENT_10','FLAG_DOCUMENT_12','FLAG_DOCUMENT_20']\n",
    "    #For building properties,we will just keep the AVG columns and remove median and mode as those are giving same infoprmation\n",
    "    #For median values of bulinding properties\n",
    "    avg= [col for col in test.columns if col.split(\"_\")[-1]=='AVG']\n",
    "    median_cols= [col for col in test.columns if col.split(\"_\")[-1]=='MEDI']\n",
    "    #for mode values of bulinding properties\n",
    "    mode_cols= [col for col in test.columns if col.split(\"_\")[-1]=='MODE']\n",
    "    non_building_mode_cols=list(set([i.split('_')[0] for i in mode_cols]).difference(set([i.split('_')[0] for i in avg])))\n",
    "\n",
    "    non_building_mode_cols= [elem+\"_MODE\" for elem in non_building_mode_cols]\n",
    "\n",
    "    #non_building_mode_cols\n",
    "\n",
    "    mode_cols = [ elem for elem in mode_cols if elem not in non_building_mode_cols ]\n",
    "\n",
    "    \n",
    "    #add these to col_to_drop\n",
    "    col_to_drop.extend(median_cols)\n",
    "    col_to_drop.extend(mode_cols)\n",
    "    #drop these columns\n",
    "    test= test.drop(col_to_drop,axis=1)\n",
    "    #step : 2. \n",
    "    \n",
    "    # here we will convert some column and also remove some outlier and make those NAN\n",
    "    ##converting age from days to years\n",
    "    test['YEARS_BIRTH'] = (-1 / 365)*test['DAYS_BIRTH']\n",
    "    #we can see in test data we dont have 'XNA' category in code_gender,So we can remove such rows,also we just had 4 rows in train dat\n",
    "    test= test[test['CODE_GENDER'] != 'XNA']\n",
    "    \n",
    "    #in DAYS_EMPLOYED we have some outliers as 365243,we need to remove it or better we can make it NAN\n",
    "    \n",
    "    test[test['DAYS_EMPLOYED']==365243]['DAYS_EMPLOYED']=np.nan\n",
    "\n",
    "    #similary we can do same for SOCIAL CIRCLE columns\n",
    "    test[test['OBS_30_CNT_SOCIAL_CIRCLE']>30]['OBS_30_CNT_SOCIAL_CIRCLE']=np.nan\n",
    "    test[test['OBS_60_CNT_SOCIAL_CIRCLE']>30]['OBS_30_CNT_SOCIAL_CIRCLE']=np.nan\n",
    "    \n",
    "    \n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a680f810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will try to do encoding for categrical data\n",
    "# Remember some of the numerical columns were seemed to be categoricals,we will incude those as well\n",
    "#from sklearn.preprocessing import LabelEncoder\n",
    "def categorical_encoding_and_fillna_continous_application(test):\n",
    "    #NAME_CONTRACT_TYPE\n",
    "    NAME_CONTRACT_TYPE_encode= {'Cash loans': 1, 'Revolving loans': 0}\n",
    "    test['NAME_CONTRACT_TYPE']= test['NAME_CONTRACT_TYPE'].map(NAME_CONTRACT_TYPE_encode)\n",
    "    \n",
    "    #CODE_GENDER\n",
    "    test['CODE_GENDER']= test['CODE_GENDER'].map({'F':1,'M':2})\n",
    "    \n",
    "    #FLAG_OWN_CAR\n",
    "    test['FLAG_OWN_CAR']= test['FLAG_OWN_CAR'].map({'N':1,'Y':0})\n",
    "    \n",
    "    #FLAG_OWN_REALTY\n",
    "    test['FLAG_OWN_REALTY']= test['FLAG_OWN_REALTY'].map({'N':0,'Y':1})\n",
    "    \n",
    "    #NAME_TYPE_SUITE\n",
    "    test['NAME_TYPE_SUITE'].fillna('Unaccompanied',inplace=True)\n",
    "\n",
    "    le_NAME_TYPE_SUITE=joblib.load(\"required_files/le_NAME_TYPE_SUITE.joblib\")\n",
    "    test['NAME_TYPE_SUITE']=le_NAME_TYPE_SUITE.transform(test['NAME_TYPE_SUITE'])\n",
    "    \n",
    "    #NAME_INCOME_TYPE\n",
    "    le_NAME_INCOME_TYPE=joblib.load(\"required_files/le_NAME_INCOME_TYPE.joblib\")\n",
    "    \n",
    "    test['NAME_INCOME_TYPE']=le_NAME_INCOME_TYPE.transform(test['NAME_INCOME_TYPE'])\n",
    "    #NAME_EDUCATION_TYPE\n",
    "    map_NAME_EDUCATION_TYPE={'Secondary / secondary special': 4,\n",
    "                             'Higher education': 2,\n",
    "                             'Incomplete higher': 3,\n",
    "                             'Lower secondary': 5,\n",
    "                             'Academic degree': 1}\n",
    "    \n",
    "    test['NAME_EDUCATION_TYPE']= test['NAME_EDUCATION_TYPE'].map(map_NAME_EDUCATION_TYPE)\n",
    "\n",
    "    #NAME_FAMILY_STATUS\n",
    "    map_NAME_FAMILY_STATUS= {'Married': 5,\n",
    "                             'Single / not married': 2,\n",
    "                             'Civil marriage': 3,\n",
    "                             'Separated': 2,\n",
    "                             'Widow': 1,\n",
    "                             'Unknown': 0}\n",
    "    test['NAME_FAMILY_STATUS']= test['NAME_FAMILY_STATUS'].map(map_NAME_FAMILY_STATUS)\n",
    "    \n",
    "    \n",
    "    #NAME_HOUSING_TYPE\n",
    "    map_NAME_HOUSING_TYPE={'House / apartment': 5,\n",
    "                       'With parents': 4,\n",
    "                       'Municipal apartment': 2,\n",
    "                       'Rented apartment': 3, 'Office apartment': 1,\n",
    "                       'Co-op apartment': 1}\n",
    "    \n",
    "    test['NAME_HOUSING_TYPE']= test['NAME_HOUSING_TYPE'].map(map_NAME_HOUSING_TYPE)\n",
    "\n",
    "    #OCCUPATION_TYPE\n",
    "    map_OCCUPATION_TYPE= {'IT staff': 0,\n",
    "                          'HR staff': 0,\n",
    "                          'Realty agents': 0,\n",
    "                          'Secretaries': 0,\n",
    "                          'Waiters/barmen staff': 0,\n",
    "                          'Private service staff': 0,\n",
    "                          'Low-skill Laborers': 1,\n",
    "                          'Cleaning staff': 1,\n",
    "                          'Accountants': 1,\n",
    "                          'Medicine staff': 2,\n",
    "                          'Cooking staff': 2,\n",
    "                          'High skill tech staff': 2,\n",
    "                          'Security staff': 2,\n",
    "                          'Managers': 3,\n",
    "                          'Core staff': 3,\n",
    "                          'Drivers': 3,\n",
    "                          'Sales staff': 4,\n",
    "                          'Laborers': 5}\n",
    "    \n",
    "    \n",
    "    test['OCCUPATION_TYPE']= test['OCCUPATION_TYPE'].map(map_OCCUPATION_TYPE)\n",
    "\n",
    "    #filling na values 1 as these occupation with no values can belong to category 1\n",
    "    test['OCCUPATION_TYPE']= test['OCCUPATION_TYPE'].fillna(1)\n",
    "    \n",
    "    #WEEKDAY_APPR_PROCESS_START: we will just drop this column because its no use\n",
    "    test.drop('WEEKDAY_APPR_PROCESS_START',axis=1,inplace=True)\n",
    "    \n",
    "    #ORGANIZATION_TYPE,we can catregorize in it in 3 \n",
    "    #ORGANIZATION_TYPE\n",
    "    map_org_type_encode=joblib.load(\"required_files/map_org_type_encode.joblib\")\n",
    "    \n",
    "    test['ORGANIZATION_TYPE']= test['ORGANIZATION_TYPE'].map(map_org_type_encode)\n",
    "\n",
    "#     #FONDKAPREMONT_MODE\n",
    "    test['FONDKAPREMONT_MODE'].fillna('Not specified',inplace=True)\n",
    "    \n",
    "    #we can label encode it\n",
    "    le_FONDKAPREMONT_MODE=joblib.load(\"required_files/le_FONDKAPREMONT_MODE.joblib\")\n",
    "    \n",
    "    test['FONDKAPREMONT_MODE']=le_FONDKAPREMONT_MODE.transform(test['FONDKAPREMONT_MODE'])\n",
    "\n",
    "    #HOUSETYPE_MODE\n",
    "    map_HOUSETYPE_MODE={'Not specified':2,'block of flats':1,'specific housing': 0,'terraced house': 0}\n",
    "    test['HOUSETYPE_MODE']= test['HOUSETYPE_MODE'].map(map_HOUSETYPE_MODE)\n",
    "\n",
    "    #filling na as 'Not specified':2\n",
    "    test['HOUSETYPE_MODE']= test['HOUSETYPE_MODE'].fillna(2)\n",
    "    \n",
    "    #WALLSMATERIAL_MODE\n",
    "    test['WALLSMATERIAL_MODE']=test['WALLSMATERIAL_MODE'].fillna(\"Not Specified\")\n",
    "\n",
    "    map_wallsmaterial_type={'Monolithic': 0,\n",
    "                       'Others': 0,\n",
    "                       'Mixed': 0,\n",
    "                       'Wooden': 1,\n",
    "                       'Block': 1,\n",
    "                       'Panel': 1,\n",
    "                       'Stone, brick': 1,\n",
    "                       'Not Specified':2}\n",
    "    test['WALLSMATERIAL_MODE']=test['WALLSMATERIAL_MODE'].map(map_wallsmaterial_type)\n",
    "\n",
    "        \n",
    "    #EMERGENCYSTATE_MODE\n",
    "    \n",
    "    test['EMERGENCYSTATE_MODE']=test['EMERGENCYSTATE_MODE'].fillna(\"No\")\n",
    "    test['EMERGENCYSTATE_MODE']=test['EMERGENCYSTATE_MODE'].map({'Yes':1,'No':0})\n",
    "    #Filling Na Values for continous columns\n",
    "    #AMT_GOODS_PRICE : fill it with same value of AMT_credit\n",
    "    test['AMT_GOODS_PRICE']=test['AMT_GOODS_PRICE'].fillna(test['AMT_CREDIT'])\n",
    "    \n",
    "    #OWN_CAR_AGE,fillna with zero because they dont have car\n",
    "    test['OWN_CAR_AGE']= test['OWN_CAR_AGE'].fillna(0)\n",
    "\n",
    "    #DAYS_LAST_PHONE_CHANGE\n",
    "    test['DAYS_LAST_PHONE_CHANGE']=test['DAYS_LAST_PHONE_CHANGE'].fillna(0)\n",
    "    #CNT_FAM_MEMBERS\n",
    "    test['CNT_FAM_MEMBERS']=test['CNT_FAM_MEMBERS'].fillna(0)\n",
    "    #AMT_ANNUITY;we suppose that where AMT_ANNUITY is equal to  AMT_CREDIT without considering interest and repayment\n",
    "    #period as 1 year\n",
    "    test['AMT_ANNUITY']=test['AMT_ANNUITY'].fillna(test['AMT_CREDIT'])\n",
    "\n",
    "    #fill na for 'EXT_SOURCE_1','EXT_SOURCE_2','EXT_SOURCE_3' with 0 i.e zero income from thate externel income source\n",
    "    test['EXT_SOURCE_1']= test['EXT_SOURCE_1'].fillna(0)\n",
    "    test['EXT_SOURCE_2']= test['EXT_SOURCE_2'].fillna(0)\n",
    "    test['EXT_SOURCE_3']= test['EXT_SOURCE_3'].fillna(0)\n",
    "    #TOTALAREA_MODE: filling it with  highest correlation colum LIVINGAREA_AVG\n",
    "    #train['TOTALAREA_MODE']= np.where(train['TOTALAREA_MODE'].isnull(), train['LIVINGAREA_AVG'], train['TOTALAREA_MODE'])\n",
    "    \n",
    "       #For Avg columns\n",
    "    building_properties_avg= [col for col in test.columns if col.split(\"_\")[-1]=='AVG']\n",
    "    for i in building_properties_avg:\n",
    "        mean= test[i].median()\n",
    "        #train[i]= train[i].fillna(mean)\n",
    "        test[i]= test[i].fillna(mean)\n",
    "    #AMT_REQ_CREDIT_BUREAU_HOUR    13.501806\n",
    "    test['AMT_REQ_CREDIT_BUREAU_HOUR'] = test['AMT_REQ_CREDIT_BUREAU_HOUR'].fillna(0)\n",
    "    # AMT_REQ_CREDIT_BUREAU_DAY     13.501806\n",
    "    test['AMT_REQ_CREDIT_BUREAU_DAY'] = test['AMT_REQ_CREDIT_BUREAU_DAY'].fillna(0)\n",
    "    # AMT_REQ_CREDIT_BUREAU_WEEK    13.501806\n",
    "    test['AMT_REQ_CREDIT_BUREAU_WEEK'] = test['AMT_REQ_CREDIT_BUREAU_WEEK'].fillna(0)\n",
    "    # AMT_REQ_CREDIT_BUREAU_MON     13.501806\n",
    "    test['AMT_REQ_CREDIT_BUREAU_MON'] = test['AMT_REQ_CREDIT_BUREAU_MON'].fillna(0)\n",
    "    # AMT_REQ_CREDIT_BUREAU_QRT     13.501806\n",
    "    test['AMT_REQ_CREDIT_BUREAU_QRT'] = test['AMT_REQ_CREDIT_BUREAU_QRT'].fillna(0)\n",
    "    # AMT_REQ_CREDIT_BUREAU_YEAR    13.501806\n",
    "    test['AMT_REQ_CREDIT_BUREAU_YEAR'] = test['AMT_REQ_CREDIT_BUREAU_YEAR'].fillna(0)\n",
    "    # OBS_30_CNT_SOCIAL_CIRCLE       0.332025\n",
    "    test['OBS_30_CNT_SOCIAL_CIRCLE'] = test['OBS_30_CNT_SOCIAL_CIRCLE'].fillna(0)\n",
    "    # DEF_30_CNT_SOCIAL_CIRCLE       0.332025\n",
    "    test['DEF_30_CNT_SOCIAL_CIRCLE'] = test['DEF_30_CNT_SOCIAL_CIRCLE'].fillna(0)\n",
    "    # OBS_60_CNT_SOCIAL_CIRCLE       0.332025\n",
    "    test['OBS_60_CNT_SOCIAL_CIRCLE'] = test['OBS_60_CNT_SOCIAL_CIRCLE'].fillna(0)\n",
    "    # DEF_60_CNT_SOCIAL_CIRCLE       0.332025\n",
    "    test['DEF_60_CNT_SOCIAL_CIRCLE'] = test['DEF_60_CNT_SOCIAL_CIRCLE'].fillna(0)\n",
    "    test['TOTALAREA_MODE']= np.where(test['TOTALAREA_MODE'].isna(), test['LIVINGAREA_AVG'], test['TOTALAREA_MODE'])\n",
    "    \n",
    "    return test  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db920260",
   "metadata": {},
   "outputs": [],
   "source": [
    "def application_data_feature_engineering(df):\n",
    "    \"\"\"\n",
    "    Here we will create some intersting features,These are basically domain knowledge and some random features\n",
    "    \"\"\"\n",
    "    #1. for client EMployment and client Birth#####################################################\n",
    "    df['PER_DAYS_EMPLOYED']= df['DAYS_EMPLOYED'] /df['DAYS_BIRTH']\n",
    "    df['DAYS_UNEMPLOYED'] = abs(df['DAYS_BIRTH'])- abs(df['DAYS_EMPLOYED'])\n",
    "    df['PER_DAYS_EMPLOYED']= df['DAYS_EMPLOYED'] /df['DAYS_BIRTH']\n",
    "    df['DAYS_UNEMPLOYED'] = abs(df['DAYS_BIRTH'])- abs(df['DAYS_EMPLOYED'])\n",
    "    df[\"OWN_CAR_AGE_RATIO\"] = df[\"OWN_CAR_AGE\"] / df[\"DAYS_BIRTH\"]\n",
    "    df[\"DAYS_ID_PUBLISHED_RATIO\"] = df[\"DAYS_ID_PUBLISH\"] / df[\"DAYS_BIRTH\"]\n",
    "    df[\"DAYS_REGISTRATION_RATIO\"] = df[\"DAYS_REGISTRATION\"] / df[\"DAYS_BIRTH\"]\n",
    "    df[\"DAYS_LAST_PHONE_CHANGE_RATIO\"] = df[\"DAYS_LAST_PHONE_CHANGE\"] / df[\"DAYS_BIRTH\"]\n",
    "    #2. clients INCOME#################################################################\n",
    "    df['CREDIT_INCOME_RATIO'] = df['AMT_CREDIT'] / (df['AMT_INCOME_TOTAL'] + 0.001)\n",
    "    df['ANNUITY_INCOME_RATIO'] = df['AMT_ANNUITY'] / (df['AMT_INCOME_TOTAL'] + 0.001)\n",
    "    df['GOODS_INCOME_RATIO'] = df['AMT_GOODS_PRICE'] / (df['AMT_INCOME_TOTAL'] + 0.001)\n",
    "    df['INCOME_EXT1_RATIO'] = df['AMT_INCOME_TOTAL'] / (df['EXT_SOURCE_1'] + 0.001)\n",
    "    df['INCOME_EXT2_RATIO'] = df['AMT_INCOME_TOTAL'] / (df['EXT_SOURCE_2'] + 0.001)\n",
    "    df['INCOME_EXT3_RATIO'] = df['AMT_INCOME_TOTAL'] / (df['EXT_SOURCE_3'] + 0.001)\n",
    "    df['INCOME_ANNUITY_DIFF'] = df['AMT_INCOME_TOTAL'] - df['AMT_ANNUITY']\n",
    "    #percentage income of person and the credit amount\n",
    "    df['INCOME_PER_CAPITA'] = df['AMT_INCOME_TOTAL'] / df['CNT_FAM_MEMBERS']\n",
    "    ########For AMT_Credit and AMT_ANNUITY####################################################\n",
    "    #percentage income of person and the credit amount\n",
    "    df['INCOME_CREDIT_RATIO'] = df['AMT_INCOME_TOTAL'] / df['AMT_CREDIT']\n",
    "    #Amount paid for previous loan appication every month decided by the number of day employed\n",
    "    df['ANNUITY_DAYS_EMPLOYED_PERC'] = df['DAYS_EMPLOYED']/ df['AMT_ANNUITY']\n",
    "    \n",
    "    df['AMT_CREDIT_DAYS_EMPLOYED_PERC'] = df['DAYS_EMPLOYED']/ df['AMT_CREDIT']\n",
    "    #Anually paid amount to amount credited\n",
    "    df['PAYMENT_RATE'] = df['AMT_ANNUITY'] / df['AMT_CREDIT']\n",
    "    \n",
    "    df['PAYMENT_RATE_INV'] = df['AMT_CREDIT'] / df['AMT_ANNUITY']\n",
    "\n",
    "    df['PAY_TOWARDS_LOAN'] = df['AMT_INCOME_TOTAL']-df['AMT_ANNUITY']\n",
    "    df['CREDIT_EXT1_RATIO'] = df['AMT_CREDIT'] / (df['EXT_SOURCE_1'] + 0.001)\n",
    "    df['CREDIT_EXT2_RATIO'] = df['AMT_CREDIT'] / (df['EXT_SOURCE_2'] + 0.001)\n",
    "    df['CREDIT_EXT3_RATIO'] = df['AMT_CREDIT'] / (df['EXT_SOURCE_2'] + 0.001)\n",
    "\n",
    "    # FOR OWN_CAR_AGE##########################################################################\n",
    "    df['CAR_EMPLOYED_DIFF'] = df['OWN_CAR_AGE'] - df['DAYS_EMPLOYED']\n",
    "    df['CAR_EMPLOYED_RATIO'] = df['OWN_CAR_AGE'] / (df['DAYS_EMPLOYED']+0.00001)\n",
    "    df['CAR_AGE_DIFF'] = abs(df['DAYS_BIRTH']) - abs(df['OWN_CAR_AGE'])\n",
    "    df['CAR_AGE_RATIO'] = df['OWN_CAR_AGE'] / (abs(df['DAYS_BIRTH']))\n",
    "    \n",
    "    #FOR Family members #############################################################################\n",
    "    df[\"CNT_ADULTS\"] = df[\"CNT_FAM_MEMBERS\"] - df[\"CNT_CHILDREN\"]\n",
    "    df['CHILDREN_RATIO'] = (df['CNT_CHILDREN']+0.0001) / (df['CNT_FAM_MEMBERS'])\n",
    "    df['LOG_CNT_CHILDREN']= np.log( df['CNT_CHILDREN'])\n",
    "    df['LOG_CNT_FAM_MEMBERS']=np.log(df['CNT_FAM_MEMBERS'])\n",
    "    #FOR FLAG DOCMENTS\n",
    "    flag_doc_cols =[col for col in df.columns if \"FLAG_DOCUMENT_\" in col ]\n",
    "    df['CNT_FLAG_DOCS']= df[flag_doc_cols].sum(axis=1)\n",
    "    # for \"NOT_LIVE\" and \"NOT WORK\",These are the column where Flag=1 if there is a adress missmatch\n",
    "    address_missmatch_cols= [col for col in df.columns if (\"NOT_LIVE\" in col) or  (\"NOT_WORK\" in col) ]\n",
    "    df['ADDRESS_MISMATCH']= df[flag_doc_cols].sum(axis=1)\n",
    "    #Even i know the AMT_REQ_CREDIT_BUREAU are no use here but will just take mean of these columns\n",
    "    AMT_Req_cb_cols= [col for col in df.columns if \"AMT_REQ_CREDIT_BUREAU\" in col ]\n",
    "    df['AMT_REQ_CREDIT_BUREAU_MEAN']= df[flag_doc_cols].mean(axis=1)\n",
    "    df['AMT_REQ_CREDIT_BUREAU_SUM']= df[flag_doc_cols].sum(axis=1)\n",
    "    df['AMT_ENQ_CREDIT_RATIO'] = df['AMT_REQ_CREDIT_BUREAU_SUM'] / (df['AMT_CREDIT'] + 0.00001)\n",
    "\n",
    "    #for Phone/Email contant\n",
    "    df['All_CONTACTS']=((df[['FLAG_EMP_PHONE','FLAG_WORK_PHONE', 'FLAG_CONT_MOBILE','FLAG_PHONE','FLAG_EMAIL']]).sum(axis=1))\n",
    "    #for days ID ,Registration changed\n",
    "    df['MAX_DAYS_CHANGED']=((df[['DAYS_ID_PUBLISH','DAYS_REGISTRATION']]).max(axis=1))\n",
    "    #EXT_SOURCE_COLUMNS\n",
    "    df['EXT_SOURCE_SUM']=(df[['EXT_SOURCE_1', 'EXT_SOURCE_2','EXT_SOURCE_3']]).sum(axis=1)\n",
    "    df['EXT_SOURCE_MEAN']=(df[['EXT_SOURCE_1', 'EXT_SOURCE_2','EXT_SOURCE_3']]).mean(axis=1)\n",
    "    \n",
    "    df['EXT_SOURCE_MEDIAN']=(df[['EXT_SOURCE_1','EXT_SOURCE_2','EXT_SOURCE_3']]).median(axis=1)\n",
    "    \n",
    "    df['EXT_SOURCE_MIN']=(df[['EXT_SOURCE_1', 'EXT_SOURCE_2','EXT_SOURCE_3']]).min(axis=1)\n",
    "    \n",
    "    df['EXT_SOURCE_MAX']=(df[['EXT_SOURCE_1', 'EXT_SOURCE_2','EXT_SOURCE_3']]).max(axis=1)\n",
    "    #BUILDING PROPERT SCORE scores,we have only considered the AVG ones because mean \n",
    "    avg= [col for col in df.columns if col.split(\"_\")[-1]=='AVG']\n",
    "    df['BUILDING_PROPERTIES_AVG_SUM'] = df[avg].sum(axis=1)\n",
    "    #client's social surroundings OBSERVED And DEFAULTED\n",
    "    social_surr_cols= [col for col in df.columns if \"CNT_SOCIAL_CIRCLE\" in col ]\n",
    "    df['CNT_SOCIAL_CIRCLE_MEAN']=df[social_surr_cols].sum(axis=1)\n",
    "    \n",
    "    #Now we will create some features based on contionous columns and aggregated by categorical columns\n",
    "    #We wont create any column which have some reference to \"TARGET\" VARIABLE,because TEST data wont have it\n",
    "    #We have the categorcial column and some continous column which are actually categorical column ,We will include both\n",
    "    # we have seen some interaction between differenet categorical variable ,SO we will try to group on some categorical varaible\n",
    "    interaction_cols_for_aggregation_on= [['OCCUPATION_TYPE','ORGANIZATION_TYPE'],\n",
    "                                     ['CODE_GENDER','NAME_CONTRACT_TYPE'],\n",
    "                                     ['FLAG_OWN_CAR','FLAG_OWN_REALTY'],\n",
    "                                     ['CODE_GENDER','FLAG_OWN_REALTY'],\n",
    "                                     ['NAME_INCOME_TYPE','NAME_EDUCATION_TYPE','CODE_GENDER'],\n",
    "                                     ['NAME_FAMILY_STATUS','NAME_HOUSING_TYPE'],\n",
    "                                     ['FONDKAPREMONT_MODE','NAME_INCOME_TYPE'],\n",
    "                                     ['HOUSETYPE_MODE','NAME_HOUSING_TYPE','NAME_INCOME_TYPE'],\n",
    "                                     ['EMERGENCYSTATE_MODE','NAME_INCOME_TYPE'],['REGION_RATING_CLIENT']]\n",
    "    \n",
    "    #Contionious column for aggregation,we are not using all we are just using some important one,\n",
    "    contiouns_cols_for_agrregation_of={'AMT_ANNUITY' : ['mean','max','min'],\n",
    "                                       'ANNUITY_INCOME_RATIO' : ['mean','max','min'],\n",
    "                                       'DAYS_UNEMPLOYED' : ['mean','min'],\n",
    "                                       'AMT_INCOME_TOTAL' : ['mean','max','min'],\n",
    "                                       'BUILDING_PROPERTIES_AVG_SUM' : ['mean','max','min'],\n",
    "                                       'EXT_SOURCE_MEAN' : ['mean','max','min'],\n",
    "                                       'EXT_SOURCE_1' : ['mean','max','min'],\n",
    "                                       'EXT_SOURCE_2' : ['mean','max','min'],\n",
    "                                       'EXT_SOURCE_3' : ['mean','max','min']}\n",
    "    \n",
    "    for group_col in interaction_cols_for_aggregation_on:\n",
    "        #grouping on categorical feature interaction\n",
    "        grouped_data= df.groupby(group_col).agg(contiouns_cols_for_agrregation_of)\n",
    "        grouped_data.columns= ['_'.join(i).upper() + '_AGG_BY_' + '_'.join(group_col) for i in grouped_data.columns]\n",
    "        #merging with data\n",
    "        df= df.join(grouped_data,on=group_col)\n",
    "    #Some Log and box cox transform\n",
    "    log_vars = [\"AMT_CREDIT\", \"AMT_INCOME_TOTAL\", \"AMT_GOODS_PRICE\", \"AMT_ANNUITY\"]\n",
    "    for i in log_vars:\n",
    "        df['LOG_'+str(i)]= np.log(abs(df[i])+1)\n",
    "    \n",
    "    box_cox_vars= ['DAYS_EMPLOYED','YEARS_BIRTH','DAYS_REGISTRATION','OWN_CAR_AGE','EXT_SOURCE_1','EXT_SOURCE_1','EXT_SOURCE_1']\n",
    "    from scipy import stats\n",
    "\n",
    "    for i in box_cox_vars:\n",
    "        df['BOXCOX_'+str(i)],_= stats.boxcox(abs(df[i])+1)\n",
    "    \n",
    "    #df= pd.get_dummies(df)\n",
    "    df.columns= [\"app_\"+col.upper() if col not in [\"TARGET\",\"SK_ID_CURR\"] else col for col in df.columns]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4881014",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_data_size(df):\n",
    "    \"\"\"\n",
    "    DUe to memory constarints we will try to reduce the data size by chnaging the datatypes of columns without loosing any information\n",
    "    \"\"\"\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4db14379",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_with_other_tables(df_test):\n",
    "    \n",
    "    bureau_merged_agg=joblib.load(\"FeatureEnginered/bureau_merged_agg.pkl\")\n",
    "    df_test=df_test.merge(bureau_merged_agg,how='left',on='SK_ID_CURR')\n",
    "    del bureau_merged_agg\n",
    "    gc.collect()\n",
    "    df_prev_app_data=joblib.load('FeatureEnginered/df_prev_app_data.pkl')\n",
    "    df_test=df_test.merge(df_prev_app_data,how='left',on='SK_ID_CURR')\n",
    "    del df_prev_app_data\n",
    "    gc.collect()\n",
    "    POS_CASH_agg=joblib.load('FeatureEnginered/POS_CASH_agg.pkl')\n",
    "    df_test=df_test.merge(POS_CASH_agg,how='left',on='SK_ID_CURR')\n",
    "    del POS_CASH_agg\n",
    "    gc.collect()\n",
    "    cc_bal_agg=joblib.load(\"FeatureEnginered/cc_bal_agg.pkl\")\n",
    "    df_test=df_test.merge(cc_bal_agg,how='left',on='SK_ID_CURR')\n",
    "    del cc_bal_agg\n",
    "    gc.collect()\n",
    "    install_pay_agg=joblib.load(\"FeatureEnginered/install_pay_agg.pkl\")\n",
    "    df_test=df_test.merge(install_pay_agg,how='left',on='SK_ID_CURR')\n",
    "    del install_pay_agg\n",
    "    gc.collect()\n",
    "    return reduce_data_size(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f83cfa02",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def imputeNa_preprocessing(df_test):\n",
    "    \n",
    "    df_test.replace([np.inf, -np.inf], np.nan,inplace=True)\n",
    "    more_than_75_per_NA_cols=joblib.load('required_files/more_than_75_per_NA_cols.pkl')\n",
    "    df_test= df_test.drop(columns=more_than_75_per_NA_cols)\n",
    "    morethan30_lessthan75_per_NA_cols=joblib.load('required_files/morethan30_lessthan75_per_NA_cols.pkl')\n",
    "    lessthan30_per_NA_cols=joblib.load('required_files/lessthan30_per_NA_cols.pkl')\n",
    "    imputer_mean=joblib.load('required_files/imputer_mean.pkl')\n",
    "    mean_imputed_df_test= imputer_mean.transform(df_test[lessthan30_per_NA_cols])\n",
    "    df_test.loc[:,lessthan30_per_NA_cols]=mean_imputed_df_test.copy()\n",
    "    del mean_imputed_df_test\n",
    "    col_lgbr_predict=joblib.load('required_files/col_lgbr_predict.pkl')\n",
    "    \n",
    "    for col in morethan30_lessthan75_per_NA_cols:\n",
    "        \n",
    "        s1= {'SK_ID_CURR'}\n",
    "        s2=set(morethan30_lessthan75_per_NA_cols)\n",
    "        s3=s2.union(s1)\n",
    "    \n",
    "        test_cols= list(set(df_test.columns)-s3)+[col]\n",
    "        test= df_test[df_test[col].isnull()][test_cols]\n",
    "        X_test=test.drop(col,axis=1).values\n",
    "        lgbr=col_lgbr_predict[col]\n",
    "        df_test.loc[df_test[col].isnull(),col]=lgbr.predict(X_test)\n",
    "        \n",
    "    del test_cols,imputer_mean,col_lgbr_predict\n",
    "    gc.collect()\n",
    "\n",
    "    return df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "159f9f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_feature_engineering(df_test):\n",
    "    \n",
    "    #previous applications columns\n",
    "    prev_app_cols= [col for col in df_test.columns if 'PREVAPP_' in col]\n",
    "\n",
    "    #For Amt_Annuity\n",
    "    prev_app_annuity_cols=[col for col in prev_app_cols if \"AMT_ANNUITY\" in col]\n",
    "\n",
    "    for col in prev_app_annuity_cols:\n",
    "        df_test[\"FINAL_\"+str(col)+\"_RATIO\"]= df_test[col]/(df_test['app_AMT_ANNUITY']+0.0001)\n",
    "        df_test[\"FINAL_\"+str(col)+\"_INCOME_RATIO\"]= df_test[col]/(df_test['app_AMT_INCOME_TOTAL']+0.0001)\n",
    "\n",
    "\n",
    "    #For AMT_CREDIT\n",
    "\n",
    "    prev_app_CREDIT_cols= [col for col in prev_app_cols if \"AMT_CREDIT\" in col]\n",
    "    for col in prev_app_CREDIT_cols:\n",
    "        df_test[\"FINAL_\"+str(col)+\"_RATIO\"]= df_test[col]/(df_test['app_AMT_CREDIT']+0.0001)\n",
    "        df_test[\"FINAL_\"+str(col)+\"_INCOME_RATIO\"]= df_test[col]/(df_test['app_AMT_INCOME_TOTAL']+0.0001)\n",
    "\n",
    "    #For AMT_GOODS\n",
    "    prev_app_GOODS_cols= [col for col in prev_app_cols if \"AMT_GOODS\" in col]\n",
    "    #There was no income column in prev_app data\n",
    "    for col in prev_app_GOODS_cols:\n",
    "        #df_test[str(col)+\"_RATIO\"]= df_test[col]/(df_test['app_AMT_GOODS_PRICE']+0.0001)\n",
    "        df_test[\"FINAL_\"+str(col)+\"_INCOME_RATIO\"]= df_test[col]/(df_test['app_AMT_INCOME_TOTAL']+0.0001)\n",
    "\n",
    "\n",
    "    #del prev_app_annuity_cols,prev_app_GOODS_cols,prev_app_CREDIT_cols\n",
    "\n",
    "    #bureau and bureau_balance columns\n",
    "\n",
    "    #we need to handle this data different ,we need to go back to EDA and see how this worked with respect to app data\n",
    "    bbl_cols= [col for col in df_test.columns if 'BBL_' in col]\n",
    "    #DAYS CREDIT\n",
    "    bbl_DAYS_CREDIT_cols=[col for col in bbl_cols if 'BBL_DAYS_CREDIT' in col and 'ENDDATE' not in col and 'UPDATE' not in col]\n",
    "    for col in bbl_DAYS_CREDIT_cols :\n",
    "        df_test[\"FINAL_\"+str(col)+\"_EMPLOYMENT_DIFF\"]= df_test[col]-df_test['app_DAYS_EMPLOYED']\n",
    "        df_test[\"FINAL_\"+str(col)+\"_REGISTRATION_DIFF\"]= df_test[col]-df_test['app_DAYS_REGISTRATION']\n",
    "\n",
    "    #AMT CREDIT_Overdue\n",
    "    bbl_AMT_CREDIT_OD_cols=[col for col in bbl_cols if 'AMT_CREDIT' in col and 'OVERDUE' in col]\n",
    "\n",
    "    for col in bbl_AMT_CREDIT_OD_cols:\n",
    "\n",
    "        df_test[\"FINAL_\"+str(col)+\"_INCOME_RATIO\"]= df_test[col]-df_test['app_AMT_INCOME_TOTAL']         \n",
    "\n",
    "    #Some other Features from Kaggle disscussions\n",
    "    df_test[\"FINAL_BBL_AMT_ANUUITY_RATIO\"]=(df_test['app_AMT_ANNUITY']+0.00001) / (df_test['BBL_AMT_ANNUITY_MEAN'] +0.00001 )\n",
    "    df_test[\"FINAL_BBL_AMT_CREDIT_RATIO\"]=(df_test['app_AMT_CREDIT']+0.00001) / (df_test['BBL_AMT_CREDIT_SUM_MEAN'] +0.00001 )\n",
    "\n",
    "\n",
    "    del bbl_cols,bbl_DAYS_CREDIT_cols,bbl_AMT_CREDIT_OD_cols\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    #credit_card_balance columns\n",
    "\n",
    "    CC_bal_cols= [col for col in df_test.columns if 'CCBAL_' in col]\n",
    "\n",
    "    #AMT_BALANCE\n",
    "\n",
    "    CC_bal_AMT_BALANCE_cols= [col for col in CC_bal_cols if 'MONTHS_BALANCE' in col]\n",
    "\n",
    "    for col in CC_bal_AMT_BALANCE_cols:\n",
    "\n",
    "        df_test[\"FINAL_\"+str(col)+\"_ANNUITY_RATIO\"]= df_test[col]/(df_test['app_AMT_ANNUITY']+0.0001)\n",
    "\n",
    "    #AMT_RECIEVABLE ,we are including here ALL recievable amounts like Prinicipal,Amount and Total Recievable\n",
    "    # ['CCBAL_AMT_RECEIVABLE_PRINCIPAL_MEAN',\n",
    "    #  'CCBAL_AMT_TOTAL_RECEIVABLE_MEAN',\n",
    "    #  'CCBAL_AMT_RECEIVABLE_SUM_MEAN']\n",
    "    CC_bal_RECEIVABLE_cols= [col for col in CC_bal_cols if 'RECEIVABLE' in col]\n",
    "\n",
    "    for col in CC_bal_RECEIVABLE_cols:\n",
    "\n",
    "        df_test[\"FINAL_\"+str(col)+\"_ANNUITY_RATIO\"]= df_test[col]/(df_test['app_AMT_ANNUITY']+0.0001)\n",
    "        df_test[\"FINAL_\"+str(col)+\"_INCOME_RATIO\"]= df_test[col]/(df_test['app_AMT_INCOME_TOTAL']+0.0001)\n",
    "\n",
    "    del CC_bal_cols,CC_bal_AMT_BALANCE_cols,CC_bal_RECEIVABLE_cols\n",
    "\n",
    "\n",
    "    #installments_payments columns\n",
    "    INSTLPAY_cols= [col for col in df_test.columns if 'INSTLPAY_' in col]\n",
    "\n",
    "\n",
    "    #AMT INSTALLMENT\n",
    "    INSTLPAY_AMT_INSTALMENT_cols= [col for col in INSTLPAY_cols if 'AMT_INSTALMENT' in col]\n",
    "\n",
    "    for col in INSTLPAY_AMT_INSTALMENT_cols:\n",
    "\n",
    "        df_test[\"FINAL_\"+str(col)+\"_INCOME_RATIO\"]= df_test[col]/(df_test['app_AMT_INCOME_TOTAL']+0.0001)\n",
    "\n",
    "\n",
    "    #AMT_PAYMENT\n",
    "\n",
    "    INSTLPAY_AMT_PAYMENT_cols= [col for col in INSTLPAY_cols if 'AMT_PAYMENT' in col]\n",
    "\n",
    "    for col in INSTLPAY_AMT_PAYMENT_cols:\n",
    "        #https://www.kaggle.com/c/home-credit-default-risk/discussion/64821\n",
    "\n",
    "        df_test[\"FINAL_\"+str(col)+\"_ANNUITY_RATIO\"]= df_test['app_AMT_ANNUITY']/(df_test[col]+0.0001)\n",
    "        df_test[\"FINAL_\"+str(col)+\"_INCOME_RATIO\"]= df_test[col]/(df_test['app_AMT_INCOME_TOTAL']+0.0001)\n",
    "\n",
    "\n",
    "    del INSTLPAY_cols,INSTLPAY_AMT_INSTALMENT_cols,INSTLPAY_AMT_PAYMENT_cols\n",
    "\n",
    "    # we can create a dummy column for CIBIL SCore\n",
    "    cibil_cols=['INSTLPAY_PAYMENT_INSTALLEMENT_NUM_DIFF_MEAN','INSTLPAY_DAYS_LATE_PAYMENT_MEAN','INSTLPAY_FLAG_LATE_PAYMENT_SUM','INSTLPAY_FLAG_LESS_PAYMENT_SUM','INSTLPAY_FLAG_NO_PAYMENT_SUM',\n",
    "                'BBL_YEAR_CREDIT_MEAN','BBL_FLAG_SECURED_LOAN_SUM','BBL_FLAG_UNSECURED_LOAN_SUM',\n",
    "                'app_AMT_REQ_CREDIT_BUREAU_WEEK']\n",
    "    df_cibil= df_test[cibil_cols]\n",
    "\n",
    "\n",
    "    scaler_cibil = MinMaxScaler(feature_range = (0, 1))\n",
    "\n",
    "    scaler_cibil.fit(df_cibil)\n",
    "\n",
    "    df_cibil=scaler_cibil.transform(df_cibil)\n",
    "\n",
    "    df_cibil= pd.DataFrame(data=df_cibil,columns=cibil_cols)\n",
    "\n",
    "    df_cibil['cibil_score']= ((0.05*df_cibil[cibil_cols[0]] + 0.05*df_cibil[cibil_cols[1]]+ 0.1*df_cibil[cibil_cols[2]]+ 0.1*df_cibil[cibil_cols[3]]+ 0.05*df_cibil[cibil_cols[4]])\n",
    "    +( 0.25*df_cibil[cibil_cols[5]]+ 0.10*df_cibil[cibil_cols[6]]+ 0.15*df_cibil[cibil_cols[7]]+\n",
    "    0.2*df_cibil[cibil_cols[8]]))*100\n",
    "\n",
    "\n",
    "    df_test['cibil_score']= df_cibil['cibil_score'].copy()\n",
    "    #df_test.replace([np.inf, -np.inf], 0,inplace=True)\n",
    "\n",
    "    del df_cibil\n",
    "    gc.collect()\n",
    "    \n",
    "    return reduce_data_size(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3df4746",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_feature_selection(df_test):\n",
    "        \n",
    "    single_value_columns=joblib.load(\"FeatureEnginered/single_value_columns.pkl\")\n",
    "    single_val_cols= [i for i in single_value_columns if i in df_test.columns]\n",
    "    #droopping those columns\n",
    "\n",
    "    df_test= df_test.drop(single_val_cols,axis=1)\n",
    "    \n",
    "    #recursive feature selection columns\n",
    "    important_features=joblib.load(\"FeatureEnginered/important_features.pkl\")\n",
    "    important_features=['SK_ID_CURR']+important_features\n",
    "    imp_feat= [i for i in important_features if i in df_test.columns]\n",
    "    left_over_features= set(important_features)-set(imp_feat)\n",
    "    df_test= df_test[imp_feat]\n",
    "    for i in left_over_features:\n",
    "        df_test[str(i)]=0\n",
    "    #important_features=\n",
    "    df_test=df_test[important_features]\n",
    "    \n",
    "    return reduce_data_size(df_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "341fdff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prepare_final(df):\n",
    "    df=preporcess_application_data(df)\n",
    "    df=categorical_encoding_and_fillna_continous_application(df)\n",
    "    df=application_data_feature_engineering(df)\n",
    "    df=merge_with_other_tables(df)\n",
    "    df=imputeNa_preprocessing(df)\n",
    "    df=final_feature_engineering(df)\n",
    "    df=final_feature_selection(df)\n",
    "    \n",
    "    return reduce_data_size(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "35cf3f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "important_feature_dict=dict()\n",
    "important_feature_dict['app_PAYMENT_RATE_INV']=\"PAYMENT_RATE: Ratio of Amount credit and Loan Annuity\"\n",
    "important_feature_dict['app_EXT_SOURCE_2']=\"EXT_SOURCE_2: Second Extra Income Source\"\n",
    "important_feature_dict['app_EXT_SOURCE_3']=\"EXT_SOURCE_3: Third Extra Income Source\"\n",
    "important_feature_dict['app_EXT_SOURCE_MAX']=\"EXT_SOURCE_MAX:Maximum of all Extra income sources\"\n",
    "important_feature_dict['app_REGION_POPULATION_RELATIVE']=\"REGION_POPULATION_RELATIVE: Population of Region where client is living\"\n",
    "important_feature_dict['INSTLPAY_INSTALLMENT_PAYMENT_DIFF_MEAN']=\"INSTALLMENT_PAYMENT_DIFF: Diff. between Amount Installment and Amount Payment\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4fa26b82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'app_PAYMENT_RATE_INV': 'PAYMENT_RATE: Ratio of Amount credit and Loan Annuity',\n",
       " 'app_EXT_SOURCE_2': 'EXT_SOURCE_2: Second Extra Income Source',\n",
       " 'app_EXT_SOURCE_3': 'EXT_SOURCE_3: Third Extra Income Source',\n",
       " 'app_EXT_SOURCE_MAX': 'EXT_SOURCE_MAX:Maximum of all Extra income sources',\n",
       " 'app_REGION_POPULATION_RELATIVE': 'REGION_POPULATION_RELATIVE: Population of Region where client is living',\n",
       " 'INSTLPAY_INSTALLMENT_PAYMENT_DIFF_MEAN': 'INSTALLMENT_PAYMENT_DIFF: Diff. between Amount Installment and Amount Payment'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "important_feature_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5b90d5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_final(random_Sample):\n",
    "    \n",
    "    id_=random_Sample['SK_ID_CURR'].values[0]\n",
    "    print(f\"The Customer id is : {id_} \\n \")\n",
    "    data=random_Sample.drop('SK_ID_CURR',axis=1)\n",
    "    lgbm_clf= joblib.load('saved_models/lgbm_clf.pkl')\n",
    "    prediction_proba=lgbm_clf.predict_proba(data)[0]\n",
    "    if lgbm_clf.predict(data)[0]==0:\n",
    "        prediction=\"Non-defaulter\"\n",
    "    else:\n",
    "        prediction=\"Defaulter\"\n",
    "    print(f\"For Cust_id : {id_} , Non defaulter probability is : {np.round(prediction_proba[0],3)} and defaulter probability is : {np.round(prediction_proba[1],3)}\")\n",
    "    print(f\"\\nSO our model predicts customer id {id_} could be  {prediction}  \\n\")\n",
    "    #feat_importances = pd.Series(lgbm_clf.feature_importances_, index=data.columns)\n",
    "    #most_imp_features=feat_importances.nlargest(10).sort_values(ascending=False).index.to_list()\n",
    "    important_feature_dict=dict()\n",
    "    important_feature_dict['app_PAYMENT_RATE_INV']=\"PAYMENT_RATE: Ratio of Amount credit and Loan Annuity\"\n",
    "    important_feature_dict['app_EXT_SOURCE_2']=\"EXT_SOURCE_2: Second Extra Income Source\"\n",
    "    important_feature_dict['app_EXT_SOURCE_3']=\"EXT_SOURCE_3: Third Extra Income Source\"\n",
    "    important_feature_dict['app_EXT_SOURCE_MAX']=\"EXT_SOURCE_MAX:Maximum of all Extra income sources\"\n",
    "    important_feature_dict['app_REGION_POPULATION_RELATIVE']=\"REGION_POPULATION_RELATIVE: Population of Region where client is living\"\n",
    "    important_feature_dict['INSTLPAY_INSTALLMENT_PAYMENT_DIFF_MEAN']=\"INSTALLMENT_PAYMENT_DIFF: Diff. between Amount Installment and Amount Payment\"\n",
    "    print(\"Most important Features values are : \\n\")\n",
    "    for i,j in important_feature_dict.items():\n",
    "        \n",
    "        print(f\"\\t\\t{j}:{data[i].values[0]} \\n \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e8d3bc",
   "metadata": {},
   "source": [
    "## Run this to run  Data prepration pipleline for model prediction in 1 shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "795e5ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data prepared for model prediction for 48744 rows in 0:01:16.168209 \n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "start=datetime.now()\n",
    "df_test=pd.read_csv(r'home-credit-default-risk/application_test.csv')\n",
    "df_test=data_prepare_final(df_test)\n",
    "joblib.dump(df_test,\"FeatureEnginered/df_test_preprocessed.pkl\")\n",
    "print(f\"Data prepared for model prediction for {df_test.shape[0]} rows in {datetime.now()-start} \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6cdee5",
   "metadata": {},
   "source": [
    "# Run this and choose any random sample of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "69ce7b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a no. between 0 to 48743 \n",
      "115\n",
      "\n",
      "\n",
      "The Customer id is : 100918 \n",
      " \n",
      "For Cust_id : 100918 , Non defaulter probability is : 0.971 and defaulter probability is : 0.029\n",
      "\n",
      "SO our model predicts customer id 100918 could be  Non-defaulter  \n",
      "\n",
      "Most important Features values are : \n",
      "\n",
      "\t\tPAYMENT_RATE: Ratio of Amount credit and Loan Annuity:15.6484375 \n",
      " \n",
      "\t\tEXT_SOURCE_2: Second Extra Income Source:0.66064453125 \n",
      " \n",
      "\t\tEXT_SOURCE_3: Third Extra Income Source:0.0 \n",
      " \n",
      "\t\tEXT_SOURCE_MAX:Maximum of all Extra income sources:0.66064453125 \n",
      " \n",
      "\t\tREGION_POPULATION_RELATIVE: Population of Region where client is living:0.010009765625 \n",
      " \n",
      "\t\tINSTALLMENT_PAYMENT_DIFF: Diff. between Amount Installment and Amount Payment:0.0 \n",
      " \n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    Sample_index= int(input(\"Enter a no. between 0 to 48743 \\n\"))\n",
    "    print(\"\\n\")\n",
    "    #getting the sample data for the choosen index\n",
    "    Sample=df_test.iloc[Sample_index:Sample_index+1,:]    \n",
    "    predict_final(Sample)\n",
    "except:\n",
    "    print(\"Please enter a number between 0 to 48743 \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
